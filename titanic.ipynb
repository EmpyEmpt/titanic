{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "titanic.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "HWvdi84ORyMG",
        "p0U6BuS2D5BR",
        "FmJw3-2eD9OD",
        "pwrzrX8FEKnM",
        "eSgbBC1qEcGN",
        "Br7i0NQLQyeU"
      ],
      "authorship_tag": "ABX9TyPcDRWe4PC+6ikjvw4sWwQn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmpyEmpt/titanic/blob/main/titanic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "HWvdi84ORyMG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaqZNUiy3ZkN"
      },
      "outputs": [],
      "source": [
        "!pip install -q sklearn\n",
        "!pip install -q tensorflow_decision_forests\n",
        "\n",
        "!pip install -q catboost\n",
        "!pip install -q scikit-learn\n",
        "%tensorflow_version 2.x  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_decision_forests as tfdf\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from catboost import CatBoostClassifier, Pool, metrics, cv\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "metadata": {
        "id": "qq-y3qzm3aRJ"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loading and prepocessing\n"
      ],
      "metadata": {
        "id": "p0U6BuS2D5BR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "uI1su2Rp4sUf"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data prepocessing"
      ],
      "metadata": {
        "id": "FmJw3-2eD9OD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.drop(['Ticket', 'Cabin', 'PassengerId'], axis=1)\n",
        "test_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\n",
        "\n",
        "train_df['Title'] = train_df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
        "test_df['Title'] = test_df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
        "\n",
        "for dataset in [train_df, test_df]:\n",
        "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n",
        " \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
        "\n",
        "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
        "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
        "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
        "    \n",
        "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
        "for dataset in [train_df, test_df]:\n",
        "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
        "    dataset['Title'] = dataset['Title'].fillna(0) \n",
        "\n",
        "train_df = train_df.drop(['Name'], axis=1)\n",
        "test_df = test_df.drop(['Name'], axis=1)\n",
        "\n",
        "train_df['Sex'] = train_df['Sex'].map({'male': 0, 'female': 1}).astype(int)\n",
        "test_df['Sex'] = test_df['Sex'].map({'male': 0, 'female': 1}).astype(int)\n",
        "\n",
        "guess_ages = np.zeros((2,3))\n",
        "for dataset in [train_df, test_df]:\n",
        "    for i in range(0, 2):\n",
        "        for j in range(0, 3):\n",
        "            guess_df = dataset[(dataset['Sex'] == i) & \\\n",
        "                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n",
        "            age_guess = guess_df.median()\n",
        "            guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5\n",
        "            \n",
        "    for i in range(0, 2):\n",
        "        for j in range(0, 3):\n",
        "            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n",
        "                    'Age'] = guess_ages[i,j]\n",
        "\n",
        "    dataset['Age'] = dataset['Age'].astype(int)\n",
        "\n",
        "train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\n",
        "train_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)\n",
        "\n",
        "for dataset in [train_df, test_df]:    \n",
        "    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n",
        "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
        "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
        "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
        "    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4\n",
        "\n",
        "train_df = train_df.drop(['AgeBand'], axis=1)\n",
        "train_df['FamilySize'] = train_df['SibSp'] + train_df['Parch'] + 1 \n",
        "test_df['FamilySize'] = test_df['SibSp'] + test_df['Parch'] + 1\n",
        "\n",
        "train_df = train_df.drop(['Parch', 'SibSp'], axis=1)\n",
        "test_df = test_df.drop(['Parch', 'SibSp'], axis=1)\n",
        "\n",
        "freq_port = train_df.Embarked.dropna().mode()[0]\n",
        "train_df['Embarked'] = train_df['Embarked'].fillna(freq_port)\n",
        "test_df['Embarked'] = test_df['Embarked'].fillna(freq_port)\n",
        "\n",
        "train_df['Embarked'] = train_df['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
        "test_df['Embarked'] = test_df['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
        "\n",
        "test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\n",
        "\n",
        "train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\n",
        "for dataset in [train_df, test_df]:\n",
        "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n",
        "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
        "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
        "    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n",
        "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
        "\n",
        "train_df = train_df.drop(['FareBand'], axis=1)"
      ],
      "metadata": {
        "id": "GsjylDk84vAB"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(dataset, test_ratio=0.20):\n",
        "  \"\"\"Splits a panda dataframe in two.\"\"\"\n",
        "  test_indices = np.random.rand(len(dataset)) < test_ratio\n",
        "  return dataset[~test_indices], dataset[test_indices]"
      ],
      "metadata": {
        "id": "rgNsznnt5y60"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, input, output, model_type):\n",
        "  if model_type is 'custom_dnn':\n",
        "    data = input.drop(columns = ['PassengerId'])\n",
        "    data = tf.convert_to_tensor(data, dtype=tf.int32)\n",
        "  elif model_type is 'catboost':\n",
        "     data = input.drop(columns = ['PassengerId'])\n",
        "  elif model_type is 'random_forest_tf':\n",
        "    data = tfdf.keras.pd_dataframe_to_tf_dataset(input)\n",
        "  elif model_type == 'random_forest_skl':\n",
        "    data = input.drop(columns = ['PassengerId'])\n",
        "\n",
        "\n",
        "  pred = model.predict(data)\n",
        "  pred_list = []\n",
        "  for i in pred:\n",
        "    if i >= 0.5:\n",
        "      pred_list.append(1)\n",
        "    else:\n",
        "      pred_list.append(0)\n",
        "  res = input['PassengerId'].to_frame()\n",
        "  res['Survived'] = pred_list\n",
        "  res.to_csv(output, index = False)"
      ],
      "metadata": {
        "id": "qlFtz_OcMmBt"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset split for quick rough evaluation"
      ],
      "metadata": {
        "id": "pwrzrX8FEKnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, test_ds = split_dataset(train_df)\n",
        "\n",
        "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds, label='Survived')\n",
        "test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_ds, label='Survived')\n",
        "\n",
        "train_, test_ = split_dataset(train_df)\n",
        "\n",
        "trainY = train_['Survived']\n",
        "trainX = train_.drop(columns = ['Survived'])\n",
        "\n",
        "testY = test_['Survived']\n",
        "testX = test_.drop(columns = ['Survived'])\n",
        "\n",
        "trainX = tf.convert_to_tensor(trainX, dtype=tf.int32)\n",
        "trainY = tf.convert_to_tensor(trainY, dtype=tf.int32)\n",
        "\n",
        "testX = tf.convert_to_tensor(testX, dtype=tf.int32)\n",
        "testY = tf.convert_to_tensor(testY, dtype=tf.int32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hKceU7WEQ-E",
        "outputId": "1ff2e89e-c564-4966-dfac-c8a825d2b50e"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_decision_forests/keras/core.py:2036: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  features_dataframe = dataframe.drop(label, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Complete dataset for final training"
      ],
      "metadata": {
        "id": "eSgbBC1qEcGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cmpl_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_df, label='Survived')\n",
        "cmpl_df = train_df.copy()\n",
        "\n",
        "cmpl_y = train_df['Survived']\n",
        "cmpl_y = tf.convert_to_tensor(cmpl_y, dtype=tf.int32)\n",
        "\n",
        "cmpl_x = train_df.drop(columns = ['Survived'])\n",
        "cmpl_x = tf.convert_to_tensor(cmpl_x, dtype=tf.int32)"
      ],
      "metadata": {
        "id": "FJBEQG4A9CSA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f13ef631-ebe2-4ca1-d641-160236ca0e4a"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_decision_forests/keras/core.py:2036: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  features_dataframe = dataframe.drop(label, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom DNN model"
      ],
      "metadata": {
        "id": "W7sjzMK5FT0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build():\n",
        "  model = Sequential()\n",
        "  input_shape = (7,)\n",
        "  model.add(Dense(49, activation = 'relu'))\n",
        "  model.add(Dense(14, activation = 'relu'))\n",
        "  model.add(Activation(\"softmax\"))\n",
        "  model.add(Dense(1))\n",
        "  return model"
      ],
      "metadata": {
        "id": "ATai0gVOZUhv"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelDNN = build()\n",
        "modelDNN.compile(optimizer='adam', loss=tf.keras.losses.MeanAbsoluteError(), metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "LlGkJB5ga90Q"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelDNN.fit(cmpl_x, \n",
        "          cmpl_y,\n",
        "          epochs=10,\n",
        "          shuffle=True,\n",
        "          verbose = 0)\n",
        "loss, acc = modelDNN.evaluate(cmpl_x, cmpl_y, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VLZ3s9lbALL",
        "outputId": "c5c9a5cf-9410-4ec6-8943-3302425b1ba2"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28/28 - 0s - loss: 0.2400 - accuracy: 0.8182 - 128ms/epoch - 5ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict(modelDNN, test_df, 'custom_dnn.csv', 'custom_dnn')"
      ],
      "metadata": {
        "id": "c2ezu6k_cZZ4"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RandomForestModel tf\n"
      ],
      "metadata": {
        "id": "KrFRV9qjnTQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelRFTF = tfdf.keras.RandomForestModel(hyperparameter_template = 'benchmark_rank1', verbose = 0)\n",
        "modelRFTF.compile(metrics=['accuracy'])\n",
        "modelRFTF.fit(cmpl_ds, verbose = 0)"
      ],
      "metadata": {
        "id": "yWcar_KQ7L5k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "037e2975-4f52-471d-f42a-b5a2d1c51bc2"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resolve hyper-parameter template \"benchmark_rank1\" to \"benchmark_rank1@v1\" -> {'winner_take_all': True, 'categorical_algorithm': 'RANDOM', 'split_axis': 'SPARSE_OBLIQUE', 'sparse_oblique_normalization': 'MIN_MAX', 'sparse_oblique_num_projections_exponent': 1.0}.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f927f0b8690>"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation = modelRFTF.evaluate(cmpl_ds, return_dict=True)\n",
        "loss, acc = modelDNN.evaluate(cmpl_x, cmpl_y, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn3sHVIH9Um-",
        "outputId": "955d409a-6909-4ecb-d67b-d8dc7fc6feea"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 157ms/step - loss: 0.0000e+00 - accuracy: 0.8687\n",
            "28/28 - 0s - loss: 0.2844 - accuracy: 0.7834 - 46ms/epoch - 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict(modelRFTF, test_df, 'random_forest_tf.csv', 'random_forest_tf')"
      ],
      "metadata": {
        "id": "NuukO9A2IFb6"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RandomForestClassifier sklearn\n"
      ],
      "metadata": {
        "id": "djRlT8Gap4LJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier()\n",
        "\n",
        "rf.fit(cmpl_x, cmpl_y)\n",
        "acc_log = round(rf.score(cmpl_x, cmpl_y) * 100, 2)\n",
        "acc_log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRyepgnHPi9F",
        "outputId": "35d97a2b-6d1b-4ec7-9d8f-dbc84d8b6c73"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88.55"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict(rf, test_df, 'random_forest_skl.csv', 'random_forest_skl')"
      ],
      "metadata": {
        "id": "D4qIjh8COWGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CatBoostClassifier "
      ],
      "metadata": {
        "id": "w0zh86KgqBve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation for catboost \n",
        "(it does not like tensors)"
      ],
      "metadata": {
        "id": "Br7i0NQLQyeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x_cat, test_x_cat, train_y_cat, test_y_cat = train_test_split(x, y, train_size=0.80, random_state=42)\n",
        "cmpl_y_cat = train_df['Survived']\n",
        "\n",
        "cmpl_x_cat = train_df.drop(columns = ['Survived'])"
      ],
      "metadata": {
        "id": "7kBjACxgUJEV"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features_indices = np.where(cmpl_x_cat.dtypes != float)[0]"
      ],
      "metadata": {
        "id": "rhVzjh9UTzST"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training catboost"
      ],
      "metadata": {
        "id": "eQJLyW0ERIGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelCat = CatBoostClassifier(\n",
        "    custom_loss=[metrics.Accuracy()],\n",
        "    random_seed=42,\n",
        "    logging_level='Silent'\n",
        ")\n",
        "\n",
        "cv_params = modelCat.get_params()\n",
        "cv_params.update({\n",
        "    'loss_function': metrics.Logloss()\n",
        "})\n",
        "\n",
        "cv_data = cv(\n",
        "    Pool(cmpl_x_cat, cmpl_y_cat, cat_features=categorical_features_indices),\n",
        "    cv_params,\n",
        "    plot=False\n",
        ") \n",
        "\n",
        "modelCat.fit(\n",
        "    cmpl_x_cat, cmpl_y_cat,\n",
        "    cat_features=categorical_features_indices,\n",
        "    eval_set=(cmpl_x_cat, cmpl_y_cat),\n",
        "    plot=False\n",
        ")"
      ],
      "metadata": {
        "id": "OFTghNm4TtNi"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best validation accuracy score: {:.2f}±{:.2f} on step {}'.format(\n",
        "    np.max(cv_data['test-Accuracy-mean']),\n",
        "    cv_data['test-Accuracy-std'][np.argmax(cv_data['test-Accuracy-mean'])],\n",
        "    np.argmax(cv_data['test-Accuracy-mean'])\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF15qTo5US_w",
        "outputId": "9cc02a73-075f-431c-9c1e-e122fa822032"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best validation accuracy score: 0.82±0.03 on step 85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict(modelCat, test_df, 'catboost.csv', 'catboost')"
      ],
      "metadata": {
        "id": "OrJvUozLLcQn"
      },
      "execution_count": 158,
      "outputs": []
    }
  ]
}